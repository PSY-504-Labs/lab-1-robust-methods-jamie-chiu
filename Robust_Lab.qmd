---
title: "Robust Methods Lab"
format: html
code-fold: true
editor: visual
execute: 
  message: false
---

# Lab 1-Robust Methods

## Instructions

-   If you are fitting a model, display the model output in a neatly formatted table. (The `gt` `tidy` and `kable` functions can help!)

-   If you are creating a plot, use `ggplot` or `base` and make sure they are publication ready. That means there are clear labels for all axes, titles, etc.

-   Commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages.

-   When you're done, we should be able to knit the final version of the QMD in your GitHub as a HTML.

    ```{r}
    #| message: false

    library(boot) # bootstrapping
    library(broom) # lm model
    library(correlation) # get different correlations
    library(data.table) # fread 
    library(datawizard) # for winsorising function
    library(easystats) # model_paramters function
    library(infer) # sample_rep_n function
    library(palmerpenguins) # penguins dataset
    library(parameters) # SE
    library(performance) # check assumptions
    library(permuco) # run permutation tests
    library(robustbase) # star data
    library(simpleboot)
    library(tidyverse)


    ```

## Robust Correlations

1.  Use the `stars` data in `robustbase`. This data looks at the relationship between temperature at the surface of a star and the light intensity.

    ```{r}
    # load in data
    stars <- robustbase::starsCYG

    ```

    a\. Plot the data and describe the pattern seen. What is Pearson's *r*?

    ```{r}
    #| echo: false
    # plot `stars` data
    stars %>% 
      ggplot(aes(log.Te, log.light)) +
      geom_point() +
      theme_classic() +
      labs(title = "Figure 1. Relationship between temperature and light intensity of a star",
           x = "Temperature",
           y = "Light Intensity")
      
      
    ```

    ```{r}
    # Pearson's r using easystats package
    correlation::correlation(stars)
    ```

    b\. Re-run the correlation, but this time use the winsorized r (20%). Do this manually and then with the correlation::correlation function from `easystats`.

    ```{r}
    # winsorize `stars` dataset
    winsorized_stars <- datawizard::winsorize(stars,
              threshold = 0.2,
              method = "percentile",
              robus = FALSE,
              verbose = TRUE)

    # rerun correlations
    # manually
    correlation(winsorized_stars)
    # with easystats::correlation
    correlation(stars, winsorize = 0.2)
    ```

    c\. Compare the correlations.

    > Answer: The correlations between light intensity and temperature became significant after winsorising the `stars` dataset. (Winsorising and then running the correlation resulted in the same outputs as running the correlation with a winsorising argument.)

## Bootstrapping and Permutations

2.  For the following data: \[8.453532, 10.025041, 11.495339, 9.367600, 8.333229, 9.788753, 10.883344, 10.543059, 9.869095, 10.799819\]

    a\. Bootstrap the mean (using the `boot` package) and plot the histogram with `ggplot2`

    ```{r}
    # create list
    sample_list <- c(8.453532, 10.025041, 11.495339, 9.367600, 8.333229, 9.788753, 10.883344, 10.543059, 9.869095, 10.799819)

    # set seed
    set.seed(8)

    # method 1: Using `simpleboot` package
    simpleboot_list <- one.boot(sample_list, mean, R=1000)
    hist(simpleboot_list)

    # method 2: Using `boot` package
    # define the function that will be used to calculate the mean
    mean_func = function(data, indices) {
      return(mean(data[indices])) #indices to do bootstrapping
    }
    # use boot to bootstrap means 1000 times
    boot_list_results <- boot(sample_list, mean_func, R=1000)
    # get means
    boot_list_means <- boot_list_results$t

    # plot histogram with ggplot
    p <- ggplot(as.data.frame(boot_list_means), aes(x = V1)) + 
      geom_histogram(binwidth = 0.1, 
                     color = "black", 
                     fill = "#1E88E5", # color-blind friendly
                     alpha = 0.4) +
      labs(title="Figure 2: Distribution of bootstrapped means",
           y="Frequency",
           x="Bootstrap Sample Means") +
      theme_classic()

    # add vline
    vline <- mean(sample_list)
    p + geom_vline(xintercept = vline,
                   color = "#D81B60") + # color-blind friendly
      annotate("text", x=vline-0.05, y=50,
               color = "#D81B60",
               label="Original sample mean", angle=90)
    ```

    b\. Bootstrap the median (using the `boot` package) and plot the histogram with `ggplot2`

    ```{r}

    # define function to calculate the median
    median_func = function(data, indices) {
      return(median(data[indices])) #indices to do bootstrapping
    }
    # set seed
    set.seed(8)
    # use boot to bootstrap medians 1000 times
    boot_list_median_results <- boot(sample_list, median_func, R=1000)
    # get medians
    boot_list_medians <- boot_list_median_results$t

    # plot histogram with ggplot
    p <- ggplot(as.data.frame(boot_list_medians), aes(x = V1)) + 
      geom_histogram(binwidth = 0.1, 
                     color = "black", 
                     fill = "#D81B60", # color-blind friendly
                     alpha = 0.4) +
      labs(title="Figure 3: Distribution of bootstrapped medians",
           y="Frequency",
           x="Bootstrap Sample Medians") +
      theme_classic()

    # add vline
    vline <- median(sample_list)
    p + geom_vline(xintercept = vline,
                   color = "#D81B60") + 
      annotate("text", x=vline-0.05, y=100,
               color = "black",
               label="Original sample median", angle=90)

    ```

    c\. For the mean bootstraps, plot the 95% confidence intervals (percentile and bca) ) along with the mean. Use `geom_vline annotate` to mark the lines noting what they represent.

    ```{r}

    boot_means_ci_perc <- boot.ci(boot_list_results, type = "perc", R=1000)
    boot_means_ci_bca <- boot.ci(boot_list_results, type = "bca", R=1000)

    # plot histogram with ggplot
    p <- ggplot(as.data.frame(boot_list_means), aes(x = V1)) + 
      geom_histogram(binwidth = 0.1, 
                     color = "black", 
                     fill = "#1E88E5", # color-blind friendly
                     alpha = 0.4) +
      labs(title="Figure 4: Distribution of bootstrapped means with 95% and BCA CI",
           y="Frequency",
           x="Bootstrap Sample Means") +
      theme_classic()

    # add vlines
    vline <- boot_means_ci_perc$percent[4:5]
    vline <- append(vline, boot_means_ci_bca$bca[4:5])
    p + geom_vline(xintercept = vline,
                   color = c("#D81B60", "#D81B60",
                             "#004D40", "#004D40")) + 
      annotate("text", x=vline[1]-0.05, y=50,
               color = "#D81B60",
               label="95% CI", angle=90) +
      annotate("text", x=vline[3]-0.05, y=100,
               color = "#004D40",
               label="BCA CI", angle=90)
    ```

    d\. For the median bootstraps, plot the 95% confidence intervals (Percentile and BCa). Use `geom_vline and annotate` to mark the lines noting what they represent.

3.  You want to test whether the following paired samples are significantly different from one another: pre = \[22,25,17,24,16,29,20,23,19,20\], post = \[18,21,16,22,19,24,17,21,23,18\]. Often researchers would run a paired sampled t-test, but you are concerned the data does not follow a normal distribution.

    a\. Calculate the paired differences, that is post - pre, which will result in a vector of paired differences (paired_diff = post - pre)

    ```{r}

    # input data
    pre <- c(22,25,17,24,16,29,20,23,19,20)
    post <- c(18,21,16,22,19,24,17,21,23,18)
    # calculate change
    paired_diff <- post - pre

    ```

    b\. Calculate the mean of the paired differences (X_paired_diff)

    ```{r}

    # calculate mean
    mean_paired_diff <- mean(paired_diff)

    ```

    c\. Bootstrap b) with replacement (pdiff1) and plot the histogram with `ggplot2`.

    ```{r}

    # bootstrap mean differences
    paired_diff_boot <- one.boot(paired_diff, mean, R=1000)
    hist(paired_diff_boot)

    ```

    d\. Calculate the 95% confidence intervals (BCa). What can you infer from this?

    ```{r}

    # calculate 95% CI using BCA
    boot.ci(paired_diff_boot, type = "bca", R=1000)
    ```

    > The 95% confidence interval ranges from -3.1 to 0.4, which means that the "true" change could be zero.

    e\. Plot bootstrap mean along with 95% CIs (with `ggplot2`). Use annotate to note what the vertical lines represent.

4.  Pepper Joe measured the length and heat of 85 chilli peppers. He wants to know if smaller peppers are hotter than longer peppers.

    ```{r}
    #read data.table to read in
    chilli <- read.delim("https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/_site/slides/03-Robust_Methods/data/chillis.csv")
    ```

    ```{r}

    # plot length + heat
    chilli %>%
      ggplot(aes(x=LENGTH, y=HEAT)) + 
      geom_point() +
      theme_classic() +
      labs(title = "Figure 5. Relationship between chilli length and heat")

    # calculate correlations
    # Pearson's r
    correlation::correlation(chilli)
    # robust correlation
    correlation::correlation(chilli, winsorize = 0.2)

    # estimate the model
    model <- lm(HEAT ~ LENGTH, data = chilli)
    tidy(model)
    # check assumptions
    check_model(model)

    # estimate robust SEs
    robust_model <- model_parameters(model, vcov = "HC3") 
    robust_model

    ```

    > Answer: A sample of 85 chillis with heat and length as parameters were fitted with a linear model. Results indicate a statistically significant negative relationship between length and heat, where a decrease in length by 0.15 was associated with a unit increase in heat (beta = -0.15, 95% CI \[-0.23, -0.07\], t(83) = -3.59, p \<.001; Std. beta = -0.37, 95% CI \[-0.57, -0.16\]; note: Robust SEs were calculated but did not change the results obtained, hence the original model estimates are reported).

5.  Some species display sexual size dimorphism -- in which one sex is on average larger than the other. Such a pattern can tell us about the species' ecology and mating habits. Do penguins display this sex difference in size? Let's just look at a subset of the palmerpenguins data set, which we'll call `my_penguins`.

    ```{r}
    my_penguins <- penguins %>% 
      filter(species == "Adelie",
             !is.na(sex), 
             island == "Torgersen") 
    head(my_penguins)
    ```

a\. Visualize body size by sex

```{r}
# plot body size by sex of penguin
my_penguins %>%
  ggplot(aes(x = as.factor(sex), y = body_mass_g, fill = as.factor(sex))) +
  geom_violin() + 
  geom_boxplot(width=0.1) +
  geom_jitter(width=0.2) +
  labs(title="Figure 6: Male and female penguin body mass",
       y="Body Mass (g)",
       x="Sex") +
  theme_classic() +
  theme(legend.position = "none")
```

b\. Calculate the original mean difference between sex

```{r}
# mean mass difference between sex
mean_mass <- my_penguins %>%
  group_by(sex) %>%
  summarize(mean_group = mean(body_mass_g)) %>%
  summarise(mean_mass_diff = diff(mean_group))
mean_mass$mean_mass_diff

```

c\. Permute the group labels (10000x)

```{r}
# permutations
sample_size <- nrow(my_penguins) # length of dataset
perm_reps   <- 10000 # number of permutations
perm_mass <- my_penguins %>%
  select(body_mass_g, sex) %>%
  rep_sample_n(size = sample_size, replace = FALSE, reps = perm_reps) %>% 
  mutate(perm_treatment = sample(sex, size = n(), replace = FALSE))  %>%
  group_by(replicate, perm_treatment)
head(perm_mass)
tail(perm_mass)

```

d\. Plot the null-hypothesis distribution (NHD) for the difference

```{r}
#| message: false
#| 
# calculate mean difference per permutation
perm_mass_diffs <- perm_mass %>%
  group_by(replicate, perm_treatment) %>%
  summarise(perm_group_means = mean(body_mass_g)) %>%
  summarise(perm_group_diffs = diff(perm_group_means))

# plot distribution of permuted mean differences
  perm_mass_diffs %>%
    ggplot(aes(x=perm_group_diffs)) +
    geom_histogram(binwidth = 30, 
                 color = "black", 
                 fill = "#1E88E5", # color-blind friendly
                 alpha = 0.4) +
  labs(title="Figure 7: Distribution of permuted mean mass differences\nin male and female penguins",
       y="Frequency",
       x="Permuted Mean Sex Differences in Mass (g)") +
    geom_vline(xintercept= mean_mass$mean_mass_diff, 
               color = "#D81B60") + 
  annotate("text", x=mean_mass$mean_mass_diff-30, y=300,
           color = "#D81B60",
           label="Original sample mean diff", angle=90)
    
  theme_classic()

```

e\. Compare the observed mean difference to the NHD (is *p* \< .05?)

6.  Suppose a replication experiment was conducted to further examine the interaction effect between driving difficulty and conversation difficulty on driving errors in a driving simulator. In the replication, the researchers administered the same three levels of conversation difficulty; (1) control, (2) easy, (3) difficult (C, E, D) but assume that they added a third level of driving difficulty; (1) low, (2) moderate, (3) difficult (L, M, D). Assume the design was completely between subjects and conduct a factorial ANOVA to test the main effects of conversation and driving difficulty as well as the interaction effect. The DV is the number of errors committed in the driving simulator.

    ```{r}
    library(tidyverse)
    fac_data<-read_csv("https://raw.githubusercontent.com/jgeller112/psy503-psych_stats/master/static/assignment/data/fact_final.csv")


    ```

    a\. Run a permutation test (ANOVA)

    b\. How would you follow-up significant effects in this context?

## Robust Linear Models

7.  Suppose we have the following data frame in R that contains information on the hours studied and exam score received by 20 students in some class:

```{r}
# input data
df <- data.frame(hours=c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4,
                         4, 5, 5, 5, 6, 6, 7, 7, 8),
                 score=c(67, 68, 74, 70, 71, 75, 80, 70, 84, 72,
                         88, 75, 95, 75, 99, 78, 99, 65, 96, 70))

```

a\. Use the lm() function to fit a regression model in R that uses **hours** as the predictor variable and **score** as the response variable

```{r}

model <- lm(score ~ hours, data=df)
tidy(model)
```

b\. Interpret the results

> Answer:
>
> A linear model was fitted to predict exam scores with hours studied using a sample of 20 students. The model was not statistically significant. From the data set, the effect of hours studied was not predictive of exam scores (beta = 1.95, 95% CI \[-0.31, 4.20\], t(18) = 1.81, p = 0.087; Std. beta = 0.39, 95% CI \[-0.06, 0.85\]).

c\. Check assumptions and report which ones failed (include plots)

```{r}
# check assumptions
check_model(model)
check_heteroscedasticity(model)
```

> Answer:
>
> Heteroscedasticity was detected, as well as the presence of influential outliers.

d\. Re-run the lm you saved above, but with robust standard errors

```{r}
robust_model <- model_parameters(model, vcov = "HC3")
robust_model
```

e\. What differences do you notice between the regular regression and the regression with robust SEs applied?
